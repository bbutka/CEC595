{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of 8b_Language_Output.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bbutka/CEC595/blob/main/8b_Language_Output.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDEziJFNRShb"
      },
      "source": [
        "Let’s write some python code to use all four of the released models for generating text. That will let us see how the changes in capacity related to the quality of the text produced.\n",
        "\n",
        "We download the GPT-2 library from OpenAI.\n",
        "\n",
        "The OpenAI codebase has a list of other libraries that it requires, which is handled by installing requirements.txt. We go to the appropriate file, requirements.txt, and install those libraries.\n",
        "\n",
        "Then, we download four different pre-trained models OpenAI made available, each roughly double in size from the previous."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bHr2nt3-ktjE",
        "outputId": "f1aba102-c192-445b-bdbb-340baedcbdd7"
      },
      "source": [
        "!git clone https://github.com/openai/gpt-2.git\n",
        "import os\n",
        "os.chdir(\"gpt-2\")\n",
        "import warnings\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
        "warnings.filterwarnings('ignore')\n",
        "%tensorflow_version 1.x\n",
        "!pip3 install -r requirements.txt\n",
        "!python3 download_model.py 124M\n",
        "!python3 download_model.py 345M\n",
        "!python3 download_model.py 774M\n",
        "!python3 download_model.py 1558M"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 233, done.\u001b[K\n",
            "remote: Total 233 (delta 0), reused 0 (delta 0), pack-reused 233\u001b[K\n",
            "Receiving objects: 100% (233/233), 4.38 MiB | 4.17 MiB/s, done.\n",
            "Resolving deltas: 100% (124/124), done.\n",
            "TensorFlow 1.x selected.\n",
            "Collecting fire>=0.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/07/a119a1aa04d37bc819940d95ed7e135a7dcca1c098123a3764a6dcace9e7/fire-0.4.0.tar.gz (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 5.0MB/s \n",
            "\u001b[?25hCollecting regex==2017.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 12.6MB/s \n",
            "\u001b[?25hCollecting requests==2.21.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/e3/20f3d364d6c8e5d2353c72a67778eb189176f08e873c9900e10c0287b84b/requests-2.21.0-py2.py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.0MB/s \n",
            "\u001b[?25hCollecting tqdm==4.31.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire>=0.1.3->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (2020.12.5)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.21.0->-r requirements.txt (line 3)) (3.0.4)\n",
            "Collecting idna<2.9,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/2c/cd551d81dbe15200be1cf41cd03869a46fe7226e7450af7a6545bfc474c9/idna-2.8-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.8MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115928 sha256=54c61c00a34b3093f536897ae5fb20117621e70fb3a8e26c7b8e45d00b27fdda\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/19/30/1ea0cad502dcb4e66ed5a690279628c827aea38bbbab75d5ed\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.4.5-cp37-cp37m-linux_x86_64.whl size=534399 sha256=6c9f0daae91c0305e864aad9b43c0d90d5fc95aa933facb36a1865538322a2a6\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement tqdm<5.0.0,>=4.38.0, but you'll have tqdm 4.31.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.21.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fbprophet 0.7.1 has requirement tqdm>=4.36.1, but you'll have tqdm 4.31.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: fire, regex, idna, requests, tqdm\n",
            "  Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Found existing installation: idna 2.10\n",
            "    Uninstalling idna-2.10:\n",
            "      Successfully uninstalled idna-2.10\n",
            "  Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed fire-0.4.0 idna-2.8 regex-2017.4.5 requests-2.21.0 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "idna",
                  "requests"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.00kit [00:00, 964kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 2.43Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 997kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:28, 17.3Mit/s]                                  \n",
            "Fetching model.ckpt.index: 6.00kit [00:00, 5.28Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 472kit [00:00, 1.46Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 1.61Mit/s]                                                       \n",
            "Fetching checkpoint: 1.00kit [00:00, 1.11Mit/s]                                                     \n",
            "Fetching encoder.json: 1.04Mit [00:00, 2.29Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 1.01Mit/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:41, 34.6Mit/s]                                 \n",
            "Fetching model.ckpt.index: 11.0kit [00:00, 8.17Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 927kit [00:00, 2.43Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 1.58Mit/s]                                                       \n",
            "Fetching checkpoint: 1.00kit [00:00, 861kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 2.17Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 889kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 3.10Git [01:33, 32.9Mit/s]                                 \n",
            "Fetching model.ckpt.index: 16.0kit [00:00, 12.2Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 1.38Mit [00:00, 2.71Mit/s]                                                \n",
            "Fetching vocab.bpe: 457kit [00:00, 1.22Mit/s]                                                       \n",
            "Fetching checkpoint: 1.00kit [00:00, 993kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 2.52Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 878kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 6.23Git [04:29, 23.1Mit/s]                                 \n",
            "Fetching model.ckpt.index: 21.0kit [00:00, 432kit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.84Mit [00:00, 3.15Mit/s]                                                \n",
            "Fetching vocab.bpe: 457kit [00:00, 1.59Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2ulVIo6RWv_"
      },
      "source": [
        "Next we import some addtional libraries we'll be using in this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLXKnOgpIX_0",
        "outputId": "5f01ed14-328c-415f-a8e9-5b9df6cde8e0"
      },
      "source": [
        "!export PYTHONIOENCODING=UTF-8\r\n",
        "os.chdir('src')\r\n",
        "\r\n",
        "!pip install tensorflow=='1.15.2'\r\n",
        "import model, sample, encoder\r\n",
        "import json\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.15.2 in /tensorflow-1.15.2/python3.7 (1.15.2)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /tensorflow-1.15.2/python3.7 (from tensorflow==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /tensorflow-1.15.2/python3.7 (from tensorflow==1.15.2) (1.15.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.2.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /tensorflow-1.15.2/python3.7 (from tensorflow==1.15.2) (1.0.8)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.12.4)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.36.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.32.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.10.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.19.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (54.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.3.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.7.4.3)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7540 sha256=cd689dd92110f36f007d72094878a040e5f852a9d25506715533a1a099fc4ddc\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "Installing collected packages: gast\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "Successfully installed gast-0.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgBnwO6w1WCd",
        "outputId": "2384dc2b-2d04-4e38-973e-9aba7cef1b3c"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\r\n",
        "tf.disable_v2_behavior()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.7/tensorflow_core/python/compat/v2_compat.py:68: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTFrok1ARZyz"
      },
      "source": [
        "We define an `autocomplete` function that returns the next `length` number of words given the `model_name` and the `raw_text` input.\n",
        "\n",
        "We set up a session for talking to the tensorflow backend. We also create a place for the output of the model to go. We checkpoint the tensorflow backend so we can establish the link to our code.Once all of that is set up, we can send our text prompt to the model for processing. We pull out the output of the model and return the string."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uckub33ZlOTR"
      },
      "source": [
        "# Return-a-string version\n",
        "\n",
        "def autocomplete(model_name, raw_text, length):\n",
        "    batch_size = 1\n",
        "    temperature = 1\n",
        "    top_k = 0\n",
        "    models_dir = '../models'\n",
        "    seed = None\n",
        "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
        "\n",
        "    enc = encoder.get_encoder(model_name, models_dir)\n",
        "    hparams = model.default_hparams()\n",
        "    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
        "        hparams.override_from_dict(json.load(f))\n",
        "\n",
        "    if length > hparams.n_ctx:\n",
        "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams.n_ctx)\n",
        "\n",
        "    with tf.Session(graph=tf.Graph()) as sess:\n",
        "        context = tf.placeholder(tf.int32, [batch_size, None])\n",
        "        np.random.seed(seed)\n",
        "        tf.set_random_seed(seed)\n",
        "        output = sample.sample_sequence(\n",
        "            hparams=hparams, length=length,\n",
        "            context=context,\n",
        "            batch_size=batch_size,\n",
        "            temperature=temperature, top_k=top_k\n",
        "        )\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
        "        saver.restore(sess, ckpt)\n",
        "\n",
        "        context_tokens = enc.encode(raw_text)\n",
        "        out = sess.run(output, feed_dict={\n",
        "                context: [context_tokens]\n",
        "        })[:, len(context_tokens):]\n",
        "        text = enc.decode(out[0])\n",
        "    return(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUrae7LcRd3z"
      },
      "source": [
        "Below is an example of our `autocomplete` function, printing out the next 10 predicted words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olLMrz6plc1n",
        "outputId": "bdf2c8b8-0a53-425f-b600-264d28019980"
      },
      "source": [
        "print(autocomplete('124M', \"Learning about machine learning is kind of like\", 10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:51: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:148: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:152: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:36: The name tf.rsqrt is deprecated. Please use tf.math.rsqrt instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:64: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:39: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:67: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "INFO:tensorflow:Restoring parameters from ../models/124M/model.ckpt\n",
            " learning how to do a tree inspection for obesity,\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPKhoX45Rin_"
      },
      "source": [
        "Here show how the predictions for a given phrase changes with the number of parameters in the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGeP35JUlb6N",
        "outputId": "dd4e8191-7938-4af7-9bbc-5c8645d90943"
      },
      "source": [
        "for gpt2model in ['124M', '345M', '774M', '1558M']:\n",
        "  print(gpt2model, autocomplete(gpt2model, \"My first time visiting the ocean, I marveled at\", 20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ../models/124M/model.ckpt\n",
            "124M  how clear seas are and the small craters of the ocean below. The depth of my sitting with\n",
            "INFO:tensorflow:Restoring parameters from ../models/345M/model.ckpt\n",
            "345M  the view, squinting at the pink and gold. The grass is covered in soft, gl\n",
            "INFO:tensorflow:Restoring parameters from ../models/774M/model.ckpt\n",
            "774M  the scale of the world. The Ammarani are famous for the length of their height, the\n",
            "INFO:tensorflow:Restoring parameters from ../models/1558M/model.ckpt\n",
            "1558M  the pure impulse behind denying obligation to creatures who don't mind doing that work for you. Only humans\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6npCZsBTbrw",
        "outputId": "bcba6eca-29ae-4d31-b3c8-eb74218d5cc2"
      },
      "source": [
        "for gpt2model in ['124M', '345M', '774M', '1558M']:\r\n",
        "  print(gpt2model, autocomplete(gpt2model, \"When I first arrived at college, I could not believe\", 20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ../models/124M/model.ckpt\n",
            "124M  that I would be part of any of the institutions that Lloyd´s Short craft commissions focus on:\n",
            "INFO:tensorflow:Restoring parameters from ../models/345M/model.ckpt\n",
            "345M  that this was going to be my field. It's now here. But he got me where I\n",
            "INFO:tensorflow:Restoring parameters from ../models/774M/model.ckpt\n",
            "774M  the real world had reduced me to an analyst. I was doing research on robots and application design thinking\n",
            "INFO:tensorflow:Restoring parameters from ../models/1558M/model.ckpt\n",
            "1558M  how much I was still limited to 20 hours a week of classes. Over the course of my final\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MJWPH-jVTHT",
        "outputId": "1e18af87-b29a-460c-f67f-9a398598ff0f"
      },
      "source": [
        "for gpt2model in ['124M', '345M', '774M', '1558M']:\r\n",
        "  print(gpt2model, autocomplete(gpt2model, \"The color of broccoli is a deep\", 1))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ../models/124M/model.ckpt\n",
            "124M  red\n",
            "INFO:tensorflow:Restoring parameters from ../models/345M/model.ckpt\n",
            "345M  golden\n",
            "INFO:tensorflow:Restoring parameters from ../models/774M/model.ckpt\n",
            "774M ,\n",
            "INFO:tensorflow:Restoring parameters from ../models/1558M/model.ckpt\n",
            "1558M ,\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}