{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of L13qs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bbutka/CEC595/blob/main/HW%236.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiXC-FZTgL_u"
      },
      "source": [
        "Use the Open Ai roulette gym\r\n",
        "\r\n",
        "Since this roulette environment has many more actions (38) than blackjack (2),  the code is designed to handle arbitary action sets. argmax is a useful routine for indicating which element of a vector contains the largest value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "obnsn2pb-ff_"
      },
      "source": [
        "import gym\n",
        "import graphviz \t\t\n",
        "from sklearn import tree\n",
        "\n",
        "def argmax(v):\n",
        "  i = 0\n",
        "  mx = v[i]\n",
        "  mxi = 0\n",
        "  while i < len(v):\n",
        "    if v[i] > mx: mx, mxi = v[i], i\n",
        "    i += 1\n",
        "  return(mxi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T43q5OAegmud"
      },
      "source": [
        "Set up the environment and some key parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-LIdH8ygUzm"
      },
      "source": [
        "env = gym.make(\"Roulette-v0\")\n",
        "gamma = 0.9\n",
        "epsilon = 0.1\n",
        "epochs = 4\n",
        "N = 10000\n",
        "gamma = 0.9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYCDjNu5gVX3"
      },
      "source": [
        "Here's the modified learner. It learns for N episodes, repeats epochs times, then displays the decision tree representing the learned value function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8phwmTeyygt"
      },
      "source": [
        "import random\r\n",
        "random.random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2HyiDINIRHj"
      },
      "source": [
        "env = gym.make(\"Roulette-v0\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  dat = []\n",
        "  lab = []\n",
        "  for _ in range(N):\n",
        "    done = False\n",
        "    observation, ret = env.reset(), 0\n",
        "    while not done:\n",
        "      # pick an action\n",
        "      if epoch == 0:\n",
        "        action = env.action_space.sample()\n",
        "      else:\n",
        "        # predict values, choose greedily\n",
        "        pred = clf.predict([(observation, a) for a in range(env.action_space.n)])\n",
        "        action = argmax(pred)\n",
        "      # epsilon greedy\n",
        "      if random.random() < epsilon:\n",
        "        action = env.action_space.sample()\n",
        "      dat += [(observation, action)]\n",
        "      observation, reward, done, info = env.step(action)\n",
        "      ret += reward\n",
        "      if done:\n",
        "        target = reward\n",
        "      elif epoch == 0:\n",
        "#        target = -1+2*random.random()\n",
        "        target = 0\n",
        "      else:\n",
        "        pred = clf.predict([(observation, a) for a in range(env.action_space.n)])\n",
        "        target = reward + gamma * max(pred)\n",
        "      lab += [target]\n",
        "  clf = tree.DecisionTreeRegressor(max_leaf_nodes = 6)\t\n",
        "  clf = clf.fit(dat, lab)\n",
        "\n",
        "  dot_data = tree.export_graphviz(clf, # class_names = [-1, 0, 1],\n",
        "                                feature_names = [\"state\", \"action\"], filled=True, rounded=True) \n",
        "  graph = graphviz.Source(dot_data)\t\n",
        "  env.close()\n",
        "graph"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}